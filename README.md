# ğŸš€ SpaceX ETL Pipeline ğŸŒŒ

Welcome to the **SpaceX ETL Pipeline** project! This repository contains an end-to-end data pipeline that extracts, transforms, and loads SpaceX launch data using Python, dbt, and Google Cloud Run.

---

## ğŸ“‹ Overview

This project is designed to:

- **Extract** data from the SpaceX API ğŸš€
- **Load** data into Google Cloud Storage (GCS) and BigQuery ğŸ“Š
- **Transform** data using dbt ğŸ§¹
- **Schedule** daily runs with Cloud Scheduler â°
- **Deploy** as a serverless container on Cloud Run for maximum cost efficiency ğŸŒ±

---

## ğŸ§  Architecture

The pipeline runs as a containerized Python application triggered daily by an HTTP request from Cloud Scheduler. The main cloud components:

- **Docker Container** ğŸ³ â€“ Bundles your DAG logic, dbt project, and extraction scripts.
- **Cloud Run** â˜ï¸ â€“ Executes the container on demand in a serverless, autoscaling environment.
- **Cloud Scheduler** ğŸ“† â€“ Triggers the Cloud Run endpoint once a day via HTTP.

---

## ğŸ› ï¸ Prerequisites

Before you begin, make sure you have:

- A **GCP project** with billing enabled ğŸ’³
- Installed the [gcloud CLI](https://cloud.google.com/sdk)
- Installed [Docker](https://www.docker.com/)
- Enabled these GCP APIs:
  - Cloud Run
  - Artifact Registry or Container Registry
  - Cloud Scheduler
  - Cloud Build (optional for CI/CD)
  - BigQuery
  - Cloud Storage

---

## ğŸš¦ Step-by-Step Deployment Guide

### âœ… Step 1: Clone the Repository

```bash
git clone https://github.com/vzucher/spacex.git
cd spacex
